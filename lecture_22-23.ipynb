{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPpKvV+ykewxej5gnjbu/Dc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["## Lecture_22 Nature Language Processing"],"metadata":{"id":"VSewBCgAgG0h"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YsfGz82IcgPX","executionInfo":{"status":"ok","timestamp":1669794653548,"user_tz":-540,"elapsed":21479,"user":{"displayName":"이용재","userId":"04954798617727159009"}},"outputId":"775064c2-e099-4833-c4f4-85f88582e36e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# 구글 드라이브 마운트.\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["from tensorflow.keras.datasets import mnist\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n","from tensorflow.keras.layers import Embedding\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import sys"],"metadata":{"id":"EjMM8d_JoLpu","executionInfo":{"status":"ok","timestamp":1669796769395,"user_tz":-540,"elapsed":7,"user":{"displayName":"이용재","userId":"04954798617727159009"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["### 텍스트의 토큰화"],"metadata":{"id":"xz7WVXyHiaeB"}},{"cell_type":"markdown","source":["#### 단어로 쪼개기"],"metadata":{"id":"O3A04zkZkHMw"}},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.text import text_to_word_sequence\n","\n","# 텍스트 전처리 부분이다. 텍스트를 작은 단위로 분해하는 것이다.\n","# 문자열을 받아서 리스트로 바꿔준다.\n","\n","text = 'Major League Baseball (MLB) is a professional baseball organization and the oldest major professional sports league in the world.'\n","result = text_to_word_sequence(text)\n","\n","print('Original Text:', text)\n","print('################')\n","print('Tokenized Text:', result)\n","print(type(result))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7cJPojL-gzip","executionInfo":{"status":"ok","timestamp":1669795230006,"user_tz":-540,"elapsed":2489,"user":{"displayName":"이용재","userId":"04954798617727159009"}},"outputId":"9f7f462c-cd7a-4205-a3be-f364facbaad9"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Original Text: Major League Baseball (MLB) is a professional baseball organization and the oldest major professional sports league in the world.\n","################\n","Tokenized Text: ['major', 'league', 'baseball', 'mlb', 'is', 'a', 'professional', 'baseball', 'organization', 'and', 'the', 'oldest', 'major', 'professional', 'sports', 'league', 'in', 'the', 'world']\n","<class 'list'>\n"]}]},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","# 단어의 빈도수 파악악\n","\n","\n","docs = ['As of 2022, a total of 30 teams play in Major League Baseball—15 teams in the National League (NL) and 15 in the American League (AL)—with 29 in the United States and 1 in Canada.'\n",", 'The NL and AL were formed in 1876 and 1901, respectively. Beginning in 1903, the two leagues signed the National Agreement and cooperated', \n","'The first few decades of professional baseball were characterized by rivalries between leagues and by players who often jumped from one team or league to another']\n","\n","token = Tokenizer()\n","token.fit_on_texts(docs)\n","print('word count: {}'.format(token.word_counts))\n","print(type(token.word_counts))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E3-aZuLZjSUn","executionInfo":{"status":"ok","timestamp":1669795423459,"user_tz":-540,"elapsed":13,"user":{"displayName":"이용재","userId":"04954798617727159009"}},"outputId":"fe329807-4706-4d67-f904-f4389ae259d4"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["word count: OrderedDict([('as', 1), ('of', 3), ('2022', 1), ('a', 1), ('total', 1), ('30', 1), ('teams', 2), ('play', 1), ('in', 7), ('major', 1), ('league', 4), ('baseball—15', 1), ('the', 7), ('national', 2), ('nl', 2), ('and', 6), ('15', 1), ('american', 1), ('al', 2), ('—with', 1), ('29', 1), ('united', 1), ('states', 1), ('1', 1), ('canada', 1), ('were', 2), ('formed', 1), ('1876', 1), ('1901', 1), ('respectively', 1), ('beginning', 1), ('1903', 1), ('two', 1), ('leagues', 2), ('signed', 1), ('agreement', 1), ('cooperated', 1), ('first', 1), ('few', 1), ('decades', 1), ('professional', 1), ('baseball', 1), ('characterized', 1), ('by', 2), ('rivalries', 1), ('between', 1), ('players', 1), ('who', 1), ('often', 1), ('jumped', 1), ('from', 1), ('one', 1), ('team', 1), ('or', 1), ('to', 1), ('another', 1)])\n","<class 'collections.OrderedDict'>\n"]}]},{"cell_type":"code","source":["#문장 개수 파악 리스트의 길이 를 알려준다. 문장내의 '.' 은 영향을 주지 않는다.\n","print('sentence count: {}'.format(token.document_count))\n","# 개별 단어들이 몇개의 문장에 쓰였는지 나타낸다. 이 경우 1~3 일 것이다.\n","print('How many sentences does each word appear in? {}'.format(token.word_docs))\n","# 개별 워드에 인덱스를 부여한다. 가장 중요한 매서드중 하나이다.\n","print('word index: {}'.format(token.word_index))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"76v1Z0VKkCK0","executionInfo":{"status":"ok","timestamp":1669795615901,"user_tz":-540,"elapsed":345,"user":{"displayName":"이용재","userId":"04954798617727159009"}},"outputId":"ec091646-3d64-4325-ca9a-af48ca634079"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["sentence count: 3\n","How many sentences does each word appear in? defaultdict(<class 'int'>, {'teams': 1, 'and': 3, '1': 1, 'as': 1, 'a': 1, 'united': 1, 'of': 2, 'total': 1, 'the': 3, 'american': 1, 'states': 1, 'al': 2, 'baseball—15': 1, '—with': 1, 'canada': 1, 'nl': 2, '2022': 1, 'major': 1, '29': 1, 'league': 2, 'national': 2, 'play': 1, '30': 1, '15': 1, 'in': 2, '1903': 1, 'signed': 1, 'were': 2, 'two': 1, 'cooperated': 1, '1876': 1, 'beginning': 1, 'respectively': 1, 'agreement': 1, 'formed': 1, '1901': 1, 'leagues': 2, 'first': 1, 'one': 1, 'between': 1, 'team': 1, 'decades': 1, 'professional': 1, 'or': 1, 'from': 1, 'jumped': 1, 'often': 1, 'players': 1, 'few': 1, 'to': 1, 'baseball': 1, 'characterized': 1, 'by': 1, 'rivalries': 1, 'another': 1, 'who': 1})\n","word index: {'in': 1, 'the': 2, 'and': 3, 'league': 4, 'of': 5, 'teams': 6, 'national': 7, 'nl': 8, 'al': 9, 'were': 10, 'leagues': 11, 'by': 12, 'as': 13, '2022': 14, 'a': 15, 'total': 16, '30': 17, 'play': 18, 'major': 19, 'baseball—15': 20, '15': 21, 'american': 22, '—with': 23, '29': 24, 'united': 25, 'states': 26, '1': 27, 'canada': 28, 'formed': 29, '1876': 30, '1901': 31, 'respectively': 32, 'beginning': 33, '1903': 34, 'two': 35, 'signed': 36, 'agreement': 37, 'cooperated': 38, 'first': 39, 'few': 40, 'decades': 41, 'professional': 42, 'baseball': 43, 'characterized': 44, 'rivalries': 45, 'between': 46, 'players': 47, 'who': 48, 'often': 49, 'jumped': 50, 'from': 51, 'one': 52, 'team': 53, 'or': 54, 'to': 55, 'another': 56}\n"]}]},{"cell_type":"markdown","source":["### 원 핫 인코딩"],"metadata":{"id":"HoI4iK6olpP1"}},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","# 텍스트 토큰화 및 인덱스 출력력\n","text = '최고가 될 때까지 끊임 없이 노력하고 최고가 되어서 떠나라.' \n","\n","token = Tokenizer()\n","token.fit_on_texts([text])\n","print('word index: {}'.format(token.word_index))\n","\n","# texts_to_squencese() 함수를 통해 토큰의 인덱스로만 채워진 배열 생성\n","x = token.texts_to_sequences([text])\n","print(x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FLzdVLnplRNq","executionInfo":{"status":"ok","timestamp":1669795979086,"user_tz":-540,"elapsed":11,"user":{"displayName":"이용재","userId":"04954798617727159009"}},"outputId":"f0c85011-08b2-4c99-fb3b-b0a154645a58"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["word index: {'최고가': 1, '될': 2, '때까지': 3, '끊임': 4, '없이': 5, '노력하고': 6, '되어서': 7, '떠나라': 8}\n","[[1, 2, 3, 4, 5, 6, 1, 7, 8]]\n"]}]},{"cell_type":"code","source":["# 원 핫 인코딩딩\n","\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","import tensorflow as tf\n","\n","text = '최고가 될 때까지 끊임 없이 노력하고 최고가 되어서 떠나라.' \n","\n","token = Tokenizer()\n","token.fit_on_texts([text])\n","#print('word index: {}'.format(token.word_index))\n","x = token.texts_to_sequences([text])\n","y = tf.keras.utils.to_categorical(x, len(token.word_index) + 1) # 길이를 줘야하는데 벡터 맨 앞에 0이 필요해서 한 칸 더 추가.\n","print(y)\n","\n","# 이런 상태로 학습하는 것은 학습이 잘 안된다. 따라서 워드 임베딩을 통해 관련성이 높은 단어들을 가까이 배치한다."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nfBDybd-mJuY","executionInfo":{"status":"ok","timestamp":1669796774681,"user_tz":-540,"elapsed":376,"user":{"displayName":"이용재","userId":"04954798617727159009"}},"outputId":"d53401c6-4aee-4daf-fbf1-8d807ffab0b9"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["[[[0. 1. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n","  [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 1.]]]\n"]}]},{"cell_type":"markdown","source":["#### 워드 임베딩"],"metadata":{"id":"BVWheQ1vncAt"}},{"cell_type":"code","source":["from tensorflow.keras.layers import Embedding\n","\n","model = Sequential()\n","model.add(Embedding(16, 4))\n","\n"],"metadata":{"id":"z_V7y4D-mfv4","executionInfo":{"status":"ok","timestamp":1669796777588,"user_tz":-540,"elapsed":3,"user":{"displayName":"이용재","userId":"04954798617727159009"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.layers import Embedding\n","\n","model = Sequential()\n","#model.add(Embedding(16, 4))\n","model.add(Embedding(16, 4, input_length = 2)) # 입력 시퀀스의 길이: 단어수는 16 개이지만 항상 두 개 씩만 넣겠다는 뜻."],"metadata":{"id":"qSl6q_lsnsjU","executionInfo":{"status":"ok","timestamp":1669796779833,"user_tz":-540,"elapsed":411,"user":{"displayName":"이용재","userId":"04954798617727159009"}}},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":["### 텍스트 읽고 긍정 부정 예측하기기"],"metadata":{"id":"sWONj9vBn-hc"}},{"cell_type":"code","source":["\n","reviews = ['너무 재밌네요', '최고에요', '참 잘 만든 영화에요', '추천하고 싶은 영화입니다', '한번 더 보고싶네요', '글쎄요', '별로에요',\n","           '생각보다 지루하네요', '연기가 어색해요', '재미없어요']\n","\n","classes = np.array([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])"],"metadata":{"id":"2ami5R7UnwEp","executionInfo":{"status":"ok","timestamp":1669796905542,"user_tz":-540,"elapsed":289,"user":{"displayName":"이용재","userId":"04954798617727159009"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["# 토큰화\n","token = Tokenizer() # new tokenizer\n","token.fit_on_texts(reviews) # text input\n","print(token.word_index)# check index\n","print(type(token.word_index))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qVuDFQaYoC6T","executionInfo":{"status":"ok","timestamp":1669796908216,"user_tz":-540,"elapsed":6,"user":{"displayName":"이용재","userId":"04954798617727159009"}},"outputId":"209d78b9-ddf6-4256-8be4-48fb83577017"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["{'너무': 1, '재밌네요': 2, '최고에요': 3, '참': 4, '잘': 5, '만든': 6, '영화에요': 7, '추천하고': 8, '싶은': 9, '영화입니다': 10, '한번': 11, '더': 12, '보고싶네요': 13, '글쎄요': 14, '별로에요': 15, '생각보다': 16, '지루하네요': 17, '연기가': 18, '어색해요': 19, '재미없어요': 20}\n","<class 'dict'>\n"]}]},{"cell_type":"code","source":["x = token.texts_to_sequences(reviews) # text to sequence\n","print(x)\n","\n","padded_x = pad_sequences(x, 4) # pad sequence\n","print(padded_x)\n","\n","word_size = len(token.word_index) + 1 # 워드 사이즈 하나 추가 해주기기\n","Embedding(word_size, 8, input_length = 4)\n","\n","model = Sequential()\n","model.add(Embedding(word_size, 8, input_length = 4))\n","model.add(Flatten())\n","model.add(Dense(1, activation = 'sigmoid'))\n","\n","model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n","model.fit(padded_x, classes, epochs = 20)\n","print('\\n Accuracy: {}'.format(model.evaluate(padded_x, classes)[1]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DxPcwW4YoWiB","executionInfo":{"status":"ok","timestamp":1669796911893,"user_tz":-540,"elapsed":1841,"user":{"displayName":"이용재","userId":"04954798617727159009"}},"outputId":"d8ec5968-41b5-4de2-d02c-26bf9e28d8c3"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["[[1, 2], [3], [4, 5, 6, 7], [8, 9, 10], [11, 12, 13], [14], [15], [16, 17], [18, 19], [20]]\n","[[ 0  0  1  2]\n"," [ 0  0  0  3]\n"," [ 4  5  6  7]\n"," [ 0  8  9 10]\n"," [ 0 11 12 13]\n"," [ 0  0  0 14]\n"," [ 0  0  0 15]\n"," [ 0  0 16 17]\n"," [ 0  0 18 19]\n"," [ 0  0  0 20]]\n","Epoch 1/20\n","1/1 [==============================] - 1s 605ms/step - loss: 0.6980 - accuracy: 0.3000\n","Epoch 2/20\n","1/1 [==============================] - 0s 10ms/step - loss: 0.6958 - accuracy: 0.3000\n","Epoch 3/20\n","1/1 [==============================] - 0s 9ms/step - loss: 0.6937 - accuracy: 0.4000\n","Epoch 4/20\n","1/1 [==============================] - 0s 8ms/step - loss: 0.6916 - accuracy: 0.5000\n","Epoch 5/20\n","1/1 [==============================] - 0s 8ms/step - loss: 0.6895 - accuracy: 0.5000\n","Epoch 6/20\n","1/1 [==============================] - 0s 12ms/step - loss: 0.6874 - accuracy: 0.5000\n","Epoch 7/20\n","1/1 [==============================] - 0s 10ms/step - loss: 0.6853 - accuracy: 0.5000\n","Epoch 8/20\n","1/1 [==============================] - 0s 8ms/step - loss: 0.6832 - accuracy: 0.8000\n","Epoch 9/20\n","1/1 [==============================] - 0s 8ms/step - loss: 0.6811 - accuracy: 0.8000\n","Epoch 10/20\n","1/1 [==============================] - 0s 12ms/step - loss: 0.6790 - accuracy: 0.8000\n","Epoch 11/20\n","1/1 [==============================] - 0s 9ms/step - loss: 0.6769 - accuracy: 0.9000\n","Epoch 12/20\n","1/1 [==============================] - 0s 10ms/step - loss: 0.6748 - accuracy: 1.0000\n","Epoch 13/20\n","1/1 [==============================] - 0s 12ms/step - loss: 0.6727 - accuracy: 1.0000\n","Epoch 14/20\n","1/1 [==============================] - 0s 10ms/step - loss: 0.6706 - accuracy: 1.0000\n","Epoch 15/20\n","1/1 [==============================] - 0s 10ms/step - loss: 0.6685 - accuracy: 1.0000\n","Epoch 16/20\n","1/1 [==============================] - 0s 8ms/step - loss: 0.6664 - accuracy: 1.0000\n","Epoch 17/20\n","1/1 [==============================] - 0s 10ms/step - loss: 0.6643 - accuracy: 1.0000\n","Epoch 18/20\n","1/1 [==============================] - 0s 10ms/step - loss: 0.6621 - accuracy: 1.0000\n","Epoch 19/20\n","1/1 [==============================] - 0s 10ms/step - loss: 0.6600 - accuracy: 1.0000\n","Epoch 20/20\n","1/1 [==============================] - 0s 9ms/step - loss: 0.6579 - accuracy: 1.0000\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_test_function.<locals>.test_function at 0x7f156dd15e60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 102ms/step - loss: 0.6557 - accuracy: 1.0000\n","\n"," Accuracy: 1.0\n"]}]},{"cell_type":"markdown","source":["#### 전체코드"],"metadata":{"id":"91FJg65rpWz5"}},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.text import text_to_word_sequence\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Flatten, Embedding\n","import numpy as np\n","\n","reviews = ['너무 재밌네요', '최고에요', '참 잘 만든 영화에요', '추천하고 싶은 영화입니다', '한번 더 보고싶네요', '글쎄요', '별로에요', '생각보다 지루하네요', '연기가 어색해요', '재미없어요']\n","classes = np.array([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])\n","\n","token = Tokenizer() # new tokenizer\n","token.fit_on_texts(reviews) # text input\n","x = token.texts_to_sequences(reviews) # text to sequence\n","padded_x = pad_sequences(x, 4)#pad sequence\n","word_size = len(token.word_index) + 1\n","\n","model = Sequential()\n","model.add(Embedding(word_size, 8, input_length = 4))\n","model.add(Flatten())\n","model.add(Dense(1, activation = 'sigmoid'))\n","\n","model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n","model.fit(padded_x, classes, epochs = 20)\n","print('\\n Accuracy: {}'.format(model.evaluate(padded_x, classes)[1]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dRwgmCVKo8BA","executionInfo":{"status":"ok","timestamp":1669796967605,"user_tz":-540,"elapsed":1313,"user":{"displayName":"이용재","userId":"04954798617727159009"}},"outputId":"9efc43f6-77af-4e1b-97de-035f7a979b6f"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","1/1 [==============================] - 0s 421ms/step - loss: 0.6990 - accuracy: 0.5000\n","Epoch 2/20\n","1/1 [==============================] - 0s 10ms/step - loss: 0.6968 - accuracy: 0.5000\n","Epoch 3/20\n","1/1 [==============================] - 0s 7ms/step - loss: 0.6946 - accuracy: 0.5000\n","Epoch 4/20\n","1/1 [==============================] - 0s 8ms/step - loss: 0.6924 - accuracy: 0.6000\n","Epoch 5/20\n","1/1 [==============================] - 0s 8ms/step - loss: 0.6903 - accuracy: 0.6000\n","Epoch 6/20\n","1/1 [==============================] - 0s 9ms/step - loss: 0.6881 - accuracy: 0.7000\n","Epoch 7/20\n","1/1 [==============================] - 0s 8ms/step - loss: 0.6860 - accuracy: 0.7000\n","Epoch 8/20\n","1/1 [==============================] - 0s 8ms/step - loss: 0.6838 - accuracy: 0.7000\n","Epoch 9/20\n","1/1 [==============================] - 0s 9ms/step - loss: 0.6817 - accuracy: 0.8000\n","Epoch 10/20\n","1/1 [==============================] - 0s 10ms/step - loss: 0.6795 - accuracy: 0.8000\n","Epoch 11/20\n","1/1 [==============================] - 0s 7ms/step - loss: 0.6774 - accuracy: 0.8000\n","Epoch 12/20\n","1/1 [==============================] - 0s 8ms/step - loss: 0.6752 - accuracy: 0.8000\n","Epoch 13/20\n","1/1 [==============================] - 0s 7ms/step - loss: 0.6730 - accuracy: 0.9000\n","Epoch 14/20\n","1/1 [==============================] - 0s 9ms/step - loss: 0.6709 - accuracy: 0.9000\n","Epoch 15/20\n","1/1 [==============================] - 0s 7ms/step - loss: 0.6687 - accuracy: 1.0000\n","Epoch 16/20\n","1/1 [==============================] - 0s 7ms/step - loss: 0.6665 - accuracy: 1.0000\n","Epoch 17/20\n","1/1 [==============================] - 0s 7ms/step - loss: 0.6643 - accuracy: 1.0000\n","Epoch 18/20\n","1/1 [==============================] - 0s 7ms/step - loss: 0.6621 - accuracy: 1.0000\n","Epoch 19/20\n","1/1 [==============================] - 0s 7ms/step - loss: 0.6599 - accuracy: 1.0000\n","Epoch 20/20\n","1/1 [==============================] - 0s 8ms/step - loss: 0.6577 - accuracy: 1.0000\n","1/1 [==============================] - 0s 102ms/step - loss: 0.6554 - accuracy: 1.0000\n","\n"," Accuracy: 1.0\n"]}]}]}